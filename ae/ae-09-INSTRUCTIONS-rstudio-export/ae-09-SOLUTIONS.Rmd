---
title: "AE 09: Bootstrapping"
subtitle: "due Thursday, November 10, 4:20pm"
author: "SOLUTIONS"
output: 
  html_document:
    toc: true
    toc_float: true
    number_section: false
    highlight: tango
    theme: "cosmo"
link-citations: yes
editor: visual
editor_options: 
  markdown: 
    wrap: sentence
  chunk_output_type: console
---

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(infer)
```

```{r}
all_polls <- read_rds("data/all_polls.rds")
```

## Exercise 1

```{r resampling-solutions}
# Compute p-hat for each poll
ex1_props <- all_polls |> 
  # Group by poll
  group_by(poll) |> 
  # Calculate proportion of yes votes
  summarize(stat = mean(vote == "yes"))
```

## Exercise 2

```{r resampling2-solution}
# Select one poll from which to resample
one_poll <- all_polls |>
  # Filter for the first poll
  filter(poll == 1) |>
  # Select vote
  select(vote)
  
# Compute p-hat* for each resampled poll
ex2_props <- one_poll |>
  # Specify vote as the response, where yes means success
  specify(response = vote, success = "yes") |>
  # Generate 1000 reps of type bootstrap
  generate(reps = 1000, type = "bootstrap") |> 
  # Calculate the summary stat "prop"
  calculate(stat = "prop")
```

## Exercise 3

```{r resampling3-solution}
# From previous steps
ex1_props <- all_polls |> 
  group_by(poll) |> 
  summarize(stat = mean(vote == "yes"))
ex2_props <- all_polls |>
  filter(poll == 1) |>
  select(vote) |>
  specify(response = vote, success = "yes") |>
  generate(reps = 1000, type = "bootstrap") |> 
  calculate(stat = "prop")
  
# Calculate variability of p-hat
ex1_props |> 
  summarize(variability = sd(stat))
  
# Calculate variability of p-hat*
ex2_props |> 
  summarize(variability = sd(stat))
```

## Exericse 4

```{r variability_phat-solution}
# Combine data from both experiments
both_ex_props <- bind_rows(ex1_props, ex2_props, .id = "experiment")
# Using both_ex_props, plot stat colored by experiment
ggplot(both_ex_props, aes(stat, color = experiment)) + 
  # Add a density layer with bandwidth 0.1
  geom_density(bw = 0.1)
```

```{r, echo = FALSE, eval = FALSE}
percentile_ci95 <- bootstrapped_props |> 
  get_confidence_interval(level = 0.95)
  
p95 <- bootstrapped_props |> 
  # Visualize in-between the endpoints given by percentile_ci
  visualize() +
  shade_confidence_interval(endpoints = percentile_ci95, direction = "between")

percentile_ci90 <- bootstrapped_props |> 
  get_confidence_interval(level = 0.90)
  
p90 <- bootstrapped_props |> 
  # Visualize in-between the endpoints given by percentile_ci
  visualize() +
  shade_confidence_interval(endpoints = percentile_ci90, direction = "between")

percentile_ci99 <- bootstrapped_props |> 
  get_confidence_interval(level = 0.99)
  
p99 <- bootstrapped_props |> 
  # Visualize in-between the endpoints given by percentile_ci
  visualize() +
  shade_confidence_interval(endpoints = percentile_ci99, direction = "between")

plot_grid(p90, p95, p99, ncol = 1)
```

## CUTTING FLOOR

### Empirical Rule

Many statistics we use in data analysis (including both the sample average and sample proportion) have nice properties that are used to better understand the population parameter(s) of interest.

One such property is that if the variability of the sample proportion (called the *standard error*, or $SE$) is known, then approximately 95% of $\hat{p}$ values (from different samples) will be within $2SE$ of the true population proportion.

To check whether that holds in the situation at hand, let's go back to the polls generated by taking many samples from the same population.

The `all_polls` dataset contains 1000 samples of size 30 from a population with a probability of voting for Candidate X equal to 0.6.

Note that you will use the R function `sd()` which calculates the variability of any set of numbers.
In statistics, when `sd()` is applied to a *variable* (e.g., price of house) we call it the *standard deviation*.
When `sd()` is applied to a *statistic* (e.g., set of sample proportions) we call it the *standard error*.

-   Run the code to generate `props`, the proportion of individuals who are planning to vote yes in each poll. *This is based upon `ex1_props` from previous exercises.*
-   Add a column, `is_in_conf_int` that is `TRUE` when the sampled proportion of yes votes is less than `2` standard errors away from the true population proportion of yes votes. That is, the `abs()`solute difference between `prop_yes` and `true_prop_yes` is less than twice `sd()` of `prop_yes`.
-   Calculate the proportion of sample statistics in the confidence interval, `prop_in_conf_int`, by taking the `mean()` of `is_in_conf_int`.

```{r emperical_rule_1, eval = FALSE}
# Proportion of yes votes by poll
props <- all_polls |> 
  group_by(poll) |> 
  summarize(prop_yes = mean(vote == "yes"))
# The true population proportion of yes votes
true_prop_yes <- 0.6
# Proportion of polls within 2SE
props |>
  # Add column: is prop_yes in 2SE of 0.6
  mutate(is_in_conf_int = abs(___ - ___) < ___ * sd(___)) |>
  # Calculate  proportion in conf int
  summarize(prop_in_conf_int = ___(___))
```

```{r emperical_rule_1-solution}
# Proportion of yes votes by poll
props <- all_polls |> 
  group_by(poll) |> 
  summarize(prop_yes = mean(vote == "yes"))
# The true population proportion of yes votes
true_prop_yes <- 0.6
# Proportion of polls within 2SE
props |>
  # Add column: is prop_yes in 2SE of 0.6
  mutate(is_in_conf_int = abs(prop_yes - true_prop_yes) < 2 * sd(prop_yes)) |>
  # Calculate  proportion in conf int
  summarize(prop_in_conf_int = mean(is_in_conf_int))
```

Great work!
In this example, it looks like 96.6% are within 2 standard errors of the true population parameter.

The image below shows that the re-sampled proportions center around $\hat{p}$, but the true proportion may be one of the values near $\hat{p}$ (red) or one of the values in the tail (blue).

```{r, echo = FALSE}
set.seed(4747)
all_polls |>
  group_by(poll) |>
  summarize(prop_yes = mean(vote == "yes")) |>
  sample_n(200) |>
  arrange(prop_yes) |>
  mutate(pointer = c(rep("col1", 90), "col2", rep("col1", 105), "col3", rep("col1", 3))) |>
ggplot() + 
  geom_dotplot(aes(x = prop_yes, fill = pointer, color = pointer), binwidth = 0.01) +
  xlab("re-sampled proportions") +
  theme(axis.title.y = element_blank(), axis.text.y.left = element_blank(),
        axis.ticks.y = element_blank(), panel.grid.major.y = element_blank(),
        panel.grid.minor = element_blank(), panel.grid.major.x = element_line(),
        legend.position = "none") +
  scale_x_continuous(breaks = seq(0.2,0.9,0.1), limits = c(0.2,0.9)) +
  scale_color_manual(values = c(openintro::COL[5,1], openintro::COL[4,1], openintro::COL[1,1])) +
  scale_fill_manual(values = c(openintro::COL[5,1], openintro::COL[4,1], openintro::COL[1,1]))

set.seed(4747)
ex2_props |>
  sample_n(200) |>
  arrange(stat) |>
  mutate(pointer = c(rep("col1", 90), "col2", rep("col1", 105), "col3", rep("col1", 3))) |>
ggplot() + 
  geom_dotplot(aes(x = stat, fill = pointer, color = pointer), binwidth = 0.01) +
  xlab("re-sampled proportions") +
  theme(axis.title.y = element_blank(), axis.text.y.left = element_blank(),
        axis.ticks.y = element_blank(), panel.grid.major.y = element_blank(),
        panel.grid.minor = element_blank(), panel.grid.major.x = element_line(),
        legend.position = "none") +
  scale_x_continuous(breaks = seq(0.2,0.9,0.1), limits = c(0.2,0.9)) +
  scale_color_manual(values = c(openintro::COL[5,1], openintro::COL[4,1], openintro::COL[1,1])) +
  scale_fill_manual(values = c(openintro::COL[5,1], openintro::COL[4,1], openintro::COL[1,1]))
```

The variability of the $\hat{p}$ statistics gives a measure for how far apart any given observed $\hat{p}$ and the parameter are expected to be.

The following code combine data from both experiments and plots the two density curves.

```{r variability_phat-solution}
# Combine data from both experiments
both_ex_props <- bind_rows(many_sample_props, bootstrapped_props, .id = "experiment")
# Using both_ex_props, plot stat colored by experiment
ggplot(both_ex_props, aes(stat, color = experiment)) + 
  # Add a density layer with bandwidth 0.1
  geom_density(bw = 0.1)
```

Remember the goal of creating a confidence interval is to find a range of plausible values for the true population parameter.

The observed statistic is a good estimate, but it is impossible to know whether it is one of the $\hat{p}$ values, which is very close to the true parameter or whether it is one of the $\hat{p}$ values, which is in the tail of the distribution of observed statistics.

We won't know how far away the statistic is from the parameter just by looking at the sample of data.

We can however, get a sense for the distance between the statistic ($\hat{p}$) and the parameter (p) by looking at how much the re-sampled (bootstrap) proportions ($\hat{p}^*$) vary.

## Exercise 4

-   Calculate $\hat{p}$ and assign the result to `p_hat`. In the call to `summarize()`, calculate `stat` as the `mean()` of `vote`s that equal `"yes"`.
-   Find an interval of values that are plausible for the true parameter by calculating $\hat{p} \pm 2SE$.
    -   The `lower` bound of the confidence interval is `p_hat` minus twice the standard error of `stat`. Use `sd()` to calculate the standard error.
    -   The `upper` bound is `p_hat` plus twice the standard error of `stat`.

<!-- Generally, if $n$ represents the size of the original sample, how many observations should we resample with replacement when bootstrapping? -->

```{r bootstrap, eval = FALSE}
p_hat <- one_poll |>
  # Calculate proportion of yes votes
  summarize(stat = ___) |>
  # Pull out as numeric vector
  pull()

# Create an interval of plausible values
bootstrapped_props |>
  summarize(
    # Lower bound is p_hat minus 2 std errors
    lower = ___,
    # Upper bound is p_hat plus 2 std errors
    upper = ___
  )
```

```{r bootstrap-solution}
# From previous exercises
one_poll <- all_polls |>
  filter(poll == 1) |>
  select(vote)
bootstrapped_props <- one_poll |>
  specify(response = vote, success = "yes") |>
  generate(reps = 1000, type = "bootstrap") |> 
  calculate(stat = "prop")
  
p_hat <- one_poll |>
  # Calculate proportion of yes votes
  summarize(stat = mean(vote == "yes")) |>
  # Pull out as numeric vector
  pull()
# Create an interval of plausible values
bootstrapped_props |>
  summarize(
    # Lower bound is p_hat minus 2 std errors
    lower = p_hat - 2 * sd(stat),
    # Upper bound is p_hat plus 2 std errors
    upper = p_hat + 2 * sd(stat)
  )
```

Remember that a confidence level describes how likely you are to have gotten a sample that was close enough to the true parameter.

## Exercise 5

Was your random sample proportion "close enough" to the true parameter? How can you tell? 

*Note: you will NOT know this in real life, because the population parameter is unknown! Which is why it's important to know these procedures work, so we can trust how often it will be right (e.g. 95% of the time) in a real research scenario.*

### Sample size effects on bootstrap CIs

Resampling with the same sample size as the original sample is important for accurately estimating the standard error and constructing the confidence intervals. If you resampled the data with the wrong size (e.g. 300 or 3 instead of 30), the standard error (SE) of the sample proportions will be off.
With 300 resampled observations, the SE is too small.
With 3 resampled observations, the SE is too large.

Here, you will use the *incorrect* standard error (based on the incorrect sample size) to create a confidence interval.
The idea is that when the standard error is off, the interval is not particularly useful, nor is it correct.

-   A function for calculating the bootstrapped t-confidence interval, `calc_t_conf_int()`, is shown is the script. Read the code and try to understand what it is doing. You can use the function by typing `calc_t_conf_int()` in the code.
-   Use the `calc_t_conf_int()` function on `bootstrapped_props` to calculate the correct t-confidence interval.
-   Do the same on `bootstrapped_props_300`, to find an incorrect interval for the resamples of size 300.
-   Do the same on `bootstrapped_props_3`, to find an incorrect interval for the resamples of size 3.

```{r bootstrap_ci-setup}
one_poll <- all_polls |>
  filter(poll == 1) |>
  select(vote)
bootstrapped_props <- one_poll |>
  rep_sample_n(30, replace = TRUE, reps = 1000) |>
  summarize(stat = mean(vote == "yes"))
  
# Resample the data using samples of size 300 (an incorrect strategy!)
bootstrapped_props_300 <- one_poll |>
  rep_sample_n(300, replace = TRUE, reps = 1000) |>
  summarize(stat = mean(vote == "yes"))
  
# Resample the data using samples of size 3 (an incorrect strategy!)
bootstrapped_props_3 <- one_poll |>
  rep_sample_n(3, replace = TRUE, reps = 1000) |>
  summarize(stat = mean(vote == "yes")) 
p_hat <- one_poll |>
  summarize(mean(vote == "yes")) |>
  pull()
```

```{r bootstrap_ci, eval = FALSE}
bootstrapped_props <- one_poll |>
  rep_sample_n(3, replace = TRUE, reps = 1000)
  # Specify vote as the response, where yes means success
  specify(response = vote, success = "yes") |>
  # Generate 1000 reps of type bootstrap
  generate(reps = 1000, type = "bootstrap") |> 
  # Calculate the summary stat "prop"
  calculate(stat = "prop")

percentile_ci <- bootstrapped_props |> 
  get_confidence_interval(level = 0.95)

calc_t_conf_int <- function(resampled_dataset) {
  resampled_dataset |>
    summarize(
      lower = p_hat - 2 * sd(stat),
      upper = p_hat + 2 * sd(stat)
    )
}
# Find the bootstrap t-confidence interval for 30 resamples
calc_t_conf_int(___)
# ... and for 300 resamples
___
# ... and for 3 resamples
___
```

```{r bootstrap_ci-solution}
calc_t_conf_int <- function(resampled_dataset) {
  resampled_dataset |>
    summarize(
      lower = p_hat - 2 * sd(stat),
      upper = p_hat + 2 * sd(stat)
    )
}
# Find the bootstrap t-confidence interval for 30 resamples
calc_t_conf_int(bootstrapped_props)
# ... and for 300 resamples
calc_t_conf_int(bootstrapped_props_300)
# ... and for 3 resamples
calc_t_conf_int(bootstrapped_props_3)
```

Notice how the resampled interval with size 300 was way too small and the resampled interval with size 3 was way too big.
Great job!

## Exercise 4

What value will the sampled $\hat{p}$ values be centered around? What about the re-sampled (bootstrapped) $\hat{p}^*$ values? 



## Exercise 4

```{r bootstrap_percentile-solution}
# From previous exercise: bootstrap t-confidence interval
ex2_props |>
  summarize(
    lower = p_hat - 2 * sd(stat),
    upper = p_hat + 2 * sd(stat)
  )
  
# Manually calculate a 95% percentile interval
ex2_props |>
  summarize(
    lower = quantile(stat, 0.025),
    upper = quantile(stat, 0.975)
  )
```

## Exercise 7

```{r bootstrap_percentile_2-solution}
# From previous step
ex2_props |>
  summarize(
    lower = quantile(stat, 0.025),
    upper = quantile(stat, 0.975)
  )
  
# Calculate the same interval, more conveniently
percentile_ci <- ex2_props |> 
  get_confidence_interval(level = 0.95)
  
# Review the value
percentile_ci
```

## Exercise 8

```{r bootstrap_percentile_3-solution, echo = FALSE}
# From previous step
percentile_ci <- ex2_props |> 
  get_confidence_interval(level = 0.95)
  
ex2_props |> 
  # Visualize in-between the endpoints given by percentile_ci
  visualize() +
  shade_confidence_interval(endpoints = percentile_ci, direction = "between")
```



-   The results you just got have been stored in a dataframe called `conf_int_data`. With this dataset, plot `ci_endpoints` (y-axis) vs. `ci_percent` (x-axis).
-   And add a line layer using `geom_line()`.

```{r bootstrap_ci_3-setup}
set.seed(47)
one_poll <- all_polls |>
  filter(poll == 1) |>
  select(vote)
  
p_hat <- one_poll |>
  summarize(mean(vote == "yes")) |>
  pull()
# Bootstrap to find the SE of p-hat: bootstrapped_props
bootstrapped_props <- one_poll |>
  specify(response = vote, success = "yes") |>
  generate(reps = 1000, type = "bootstrap") |> 
  calculate(stat = "prop")
  

```

```{r bootstrap_ci_3, eval = FALSE}
# Calculate a 95% bootstrap percentile interval
bootstrapped_props |> 
  ___(___) 
# Calculate a 99% bootstrap percentile interval
___ |> 
  ___(___) 
# Calculate a 90% bootstrap percentile interval
___ |> 
  ___(___) 



# Plot ci_endpoints vs. ci_percent to compare the intervals
ggplot(conf_int_data, aes(___, ___)) +
  # Add a line layer
  ___()
```


### Motivating CIs

Remember that the goal is to find an interval estimate of the parameter: the true proportion who will vote for candidate X, when the only information is the sample, which gives the proportion of individuals in the data who will vote for candidate X.

Ideas about the variability of $\hat{p}^*$ are used to estimate how far the observed $\hat{p}$ was from the population proportion.

Because we don't know whether the sample is close to the population or far from it, we don't know whether the confidence interval actually captures the true parameter.
